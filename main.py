from tokenize import String
from array import *
import numpy as np
import torch
import torch.nn as nn
from numpy import var
from torch.autograd import Variable
import torch.utils.data as Data
import torchvision
import matplotlib.pyplot as plt

torch.manual_seed(1)  # reproducible
# Hyper Parameters
EPOCH = 1  # training round
BATCH_SIZE = 50
LR = 0.001  # learning rate
DOWNLOAD_MNIST = True  # Set this to False if you have already downloaded the dataset
a=[]
b=[]

# Mnist digits dataset
train_data = torchvision.datasets.MNIST(
    root='./mnist/',
    train=True,  # this is training data
    transform=torchvision.transforms.ToTensor(),  # Converts a PIL.Image or numpy.ndarray to
    download=DOWNLOAD_MNIST,  # download it if you don't have it
)

examples = enumerate(train_data)
batch_idx, (example_data, example_targets) = next(examples)
print(example_targets)
print(example_data.shape)

# plot one example
print(train_data.train_data.size())  # (60000, 28, 28)
print(train_data.train_labels.size())  # (60000)
# plt.imshow(train_data.train_data[0].numpy(), cmap='gray')  # 打印出数据集中第一个手写数字的灰度图片
# plt.title('%i' % train_data.train_labels[0])
# plt.show()

# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)
shuffle=True
train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)
# convert test data into Variable, pick 2000 samples to speed up testing
test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)  # --->测试数据
test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000] / 255.
# shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)
test_y = test_data.test_labels[:2000]


class CNN(nn.Module):  # 继承了nn.Model
    def __init__(self):
        # It inherits the properties of the CNN parent class
        super(CNN, self).__init__()
        # define first convolution layer
        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)
            nn.Conv2d(  # 这里的nn.Conv2d使用一个2维度卷积
                in_channels=1,  # in_channels：在文本应用中，即为词向量的维度
                out_channels=24,  # out_channels：The number of channels generated by the convolution, the number of out_channels, is the number of one-dimensional convolution (that is, the number of convolution kernels)
                kernel_size=5,
                stride=1,  #
                padding=2,  # padding：For each edge of the input, add a layer of 0
            ),  # output shape (24, 28, 28)
            nn.ReLU(),    # activation
            # in 2X2 pooling,to choose the max number
            nn.MaxPool2d(kernel_size=2),  # choose max value in 2x2 area, output shape (16, 14, 14)
        )
        # define second convolution layer
        self.conv2 = nn.Sequential(  # input shape (16,14,14)
            nn.Conv2d(24, 48, 5, 1, 2),  # output shape (48, 14, 14)
            nn.ReLU(),  # activation
            nn.MaxPool2d(2),)  # output shape (48, 7, 7)
        #when I went to add the third layer,I find the size of photo that 7*7 can not pool again ,so I do not know how to solve it
        # self.conv3 = nn.Sequential(  # input shape (1, 28, 28)
        #     nn.Conv2d(48, 64, 5, 1, 2),  # output shape (32, 14, 14)
        #     nn.ReLU(),  # activation
        #     nn.MaxPool2d(2),)
        #
        self.out = nn.Linear(48 * 7 * 7, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        # x = self.conv3(x)
        x = x.view(x.size(0), -1)  # flatten the output of conv2 to (batch_size, 32 * 7 * 7)
        output = self.out(x)
        # Output is our real value, and x is the parameter used for data visualization
        return output, x  # return x for visualization


# 把CNN神经网络类实例化
cnn = CNN()
print(cnn)  # net architecture
# Set optimizer
optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)  # optimize all cnn parameters
#  Set loss function
loss_func = nn.CrossEntropyLoss()  # the target label is not one-hotted
# following function (plot_with_labels) is for visualization, can be ignored if not interested
from matplotlib import cm

try:
    from sklearn.manifold import TSNE; HAS_SK = True
except:
    HAS_SK = False; print('Please install sklearn for layer visualization')


def plot_with_labels(lowDWeights, labels):
    plt.cla()
    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]
    for x, y, s in zip(X, Y, labels):
        c = cm.rainbow(int(255 * s / 9));
        plt.text(x, y, s, backgroundcolor=c, fontsize=9)
    plt.xlim(X.min(), X.max());
    plt.ylim(Y.min(), Y.max());
    plt.title('Visualize last layer');
    plt.show();
    plt.pause(0.01)

plt.ion()

# training and testing
for epoch in range(EPOCH):
    c = BATCH_SIZE
    # fig = plt.figure()
    # Batch process data when iterating through our data set
    # step is the number of cycles, (x,y) corresponding to target and true in each train_loader data
    for step, (x, y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader
        # Variable 类型可以对该类型的数据自动求导
        b_x = Variable(x)  # batch x
        b_y = Variable(y)  # batch y
        output = cnn(b_x)[0]  # cnn output
        loss = loss_func(output, b_y)  # cross entropy loss
        optimizer.zero_grad()  # clear gradients for this training step
        loss.backward()  # backpropagation, compute gradients
        optimizer.step()  # apply gradients
        if step % 50 == 0:
            # Test with test set data, and get two return values one for data visualization and one for real
            test_output, last_layer = cnn(test_x)
            # Select the largest value as the predicted value when the true value is returned
            pred_y = torch.max(test_output, 1)[1].data.squeeze()
            # Here test_y is the gray image itself tag accuracy is all the predicted value == tag (the number of times the test set predicted success)/ all the data in the test set, that is, the prediction success rate
            accuracy = sum(pred_y == test_y) / float(test_y.size(0))
            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.item(), '| test accuracy: %.2f' % accuracy)
            a.append(c)
            b.append(loss.data.item())
            c=c+50
            # print(a)
            # print(b)
            if HAS_SK:
                # Visualization of trained flatten layer (T-SNE)
                tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
                plot_only = 500
                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])
                labels = test_y.numpy()[:plot_only]
                plot_with_labels(low_dim_embs, labels)
plt.ioff()
plt.plot(a,b, color='blue')
plt.legend(['Train Loss'], loc='upper right')
plt.xlabel('number of training examples seen')
plt.ylabel('negative log likelihood loss')
plt.show()

# print 6 predictions from test data


test_output, _ = cnn(test_x[:6])
pred_y = torch.max(test_output, 1)[1].data.numpy()
real_y=test_y[:6].numpy()
real_x=torch.from_numpy(real_y)
for i in range(6):
  # print( 'prediction number',pred_y[i],'real number',real_y[i])
  plt.subplot(2,3, i + 1)
  plt.tight_layout()
  realarray=np.array(real_x[i])
  # print(test_x.size())
  plt.imshow(test_x[i,0].numpy(), cmap='gray')
  n=int(pred_y[i])
  a=str(n)
  name="Prediction:"+a
  plt.title(name,color='blue')
  plt.xticks([])
  plt.yticks([])
plt.show()

